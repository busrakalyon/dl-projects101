{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a88a2ba7",
      "metadata": {
        "id": "a88a2ba7"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fa04e58",
      "metadata": {
        "id": "7fa04e58"
      },
      "source": [
        "### 1) Load Dataset and Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30d5ab5",
      "metadata": {
        "id": "e30d5ab5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"garythung/trashnet\", cache_dir=\"/content/cache\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3814a72",
      "metadata": {
        "id": "d3814a72"
      },
      "outputs": [],
      "source": [
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7d8483",
      "metadata": {
        "id": "af7d8483"
      },
      "outputs": [],
      "source": [
        "# HF Dataset üzerinde Stratify benzeri split (class distribution korunur)\n",
        "ds_train_test = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
        "ds_train_valid = ds_train_test['train'].train_test_split(test_size=0.125, seed=42)  # ~70/15/15\n",
        "\n",
        "train_raw = ds_train_valid['train']\n",
        "val_raw   = ds_train_valid['test']\n",
        "test_raw  = ds_train_test['test']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdcefbff",
      "metadata": {
        "id": "bdcefbff"
      },
      "source": [
        "Validation veri kümesi hiperparametre ayarı, erken durdurma ve model seçimi için kullanılır."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f8ecf6",
      "metadata": {
        "id": "55f8ecf6"
      },
      "source": [
        "### 2) Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfd8042b",
      "metadata": {
        "id": "dfd8042b"
      },
      "outputs": [],
      "source": [
        "INPUT_SIZE = (64, 64)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),   # gri tonlamaya dönüştürme\n",
        "    transforms.Resize(INPUT_SIZE, antialias=True),\n",
        "    transforms.ToTensor(),                         # pikselleri [0, 1] aralığına getir\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])    # [-1, 1] aralığına normalleştirme\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59392cc7",
      "metadata": {
        "id": "59392cc7"
      },
      "outputs": [],
      "source": [
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize(INPUT_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdbc831e",
      "metadata": {
        "id": "fdbc831e"
      },
      "source": [
        "Transform ile Data Augmentation yapılarak modelin daha çok veri ile ile beslenip daha iyi genelleme yapması amaçlanıyor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f3aeede",
      "metadata": {
        "id": "1f3aeede"
      },
      "source": [
        "### 3) Dataset Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "215a2f4d",
      "metadata": {
        "id": "215a2f4d"
      },
      "outputs": [],
      "source": [
        "# HF Dataset'i PyTorch DataLoader ile kullanılabilir hale getirme\n",
        "# Bu, DataLoader'ın PyTorch modeline uygun şekilde veri sağlamasını sağlar.\n",
        "class HFDatasetWrapper(Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None):\n",
        "        self.ds = hf_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.ds[idx]\n",
        "        image = item['image']   # PIL image\n",
        "        label = item['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "271dff64",
      "metadata": {
        "id": "271dff64"
      },
      "outputs": [],
      "source": [
        "# Pytorch Dataset'leri\n",
        "train_ds = HFDatasetWrapper(train_raw, transform=train_transform)\n",
        "val_ds   = HFDatasetWrapper(val_raw,   transform=val_test_transform)\n",
        "test_ds  = HFDatasetWrapper(test_raw,  transform=val_test_transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1fe0449",
      "metadata": {
        "id": "b1fe0449"
      },
      "source": [
        "### 4) DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r /content/drive/MyDrive/veri_kaynak /content/\n"
      ],
      "metadata": {
        "id": "1YmtYncvHBk9"
      },
      "id": "1YmtYncvHBk9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a87a281",
      "metadata": {
        "id": "6a87a281"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "batch_size = 24\n",
        "\n",
        "\n",
        "num_workers = 2\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                         num_workers=num_workers, pin_memory=True,\n",
        "                         persistent_workers=True, drop_last=True, prefetch_factor=2)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
        "                         num_workers=num_workers, pin_memory=True,\n",
        "                         persistent_workers=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
        "                         num_workers=num_workers, pin_memory=True,\n",
        "                         persistent_workers=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a057512b",
      "metadata": {
        "id": "a057512b"
      },
      "source": [
        "### 5) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072124a6",
      "metadata": {
        "id": "072124a6"
      },
      "outputs": [],
      "source": [
        "class SimpleTrashCNN(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=6, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 16, 3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 128 -> 64\n",
        "\n",
        "            nn.Conv2d(16, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # 64 -> 32\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),   # 32x32 -> 1x1\n",
        "            nn.Flatten(),                  # 32 * 1 * 1 = 32\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(32, num_classes)     # <-- Burada 64 yerine 32\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "752ec406",
      "metadata": {
        "id": "752ec406"
      },
      "source": [
        "### 6) Loss, Optimizer, Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7452044a",
      "metadata": {
        "id": "7452044a"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Kullanılan cihaz:\", device)\n",
        "model = SimpleTrashCNN(in_channels=1, num_classes=6).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d945b97a",
      "metadata": {
        "id": "d945b97a"
      },
      "source": [
        "### 7) Early Stopping Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331231a4",
      "metadata": {
        "id": "331231a4"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, save_path=\"best_model.pth\"):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.early_stop = False\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            torch.save(model.state_dict(), self.save_path)\n",
        "            print(f\"Model saved: {self.save_path}\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f\"Early stopped counter: {self.counter}/{self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dab7a28",
      "metadata": {
        "id": "1dab7a28"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(patience=5, save_path=\"/content/drive/MyDrive/best_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c40415",
      "metadata": {
        "id": "07c40415"
      },
      "source": [
        "### 8) Train, Eval, Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "620d75ca",
      "metadata": {
        "id": "620d75ca"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Daha verimli CPU aktarımı\n",
        "            all_preds.append(preds.detach().cpu())\n",
        "            all_labels.append(labels.detach().cpu())\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "\n",
        "    # Liste yerine tek tensor halinde döndürmek daha verimli\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    return avg_loss, acc, all_preds, all_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import autocast\n",
        "scaler = torch.amp.GradScaler('cuda')"
      ],
      "metadata": {
        "id": "toZzJw20rrj0"
      },
      "id": "toZzJw20rrj0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUYXkURTbTXe"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, early_stopping, epochs=10):\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epochs\", dynamic_ncols=True):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        batch_bar = tqdm(train_loader, leave=False, dynamic_ncols=True, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "        for images, labels in batch_bar:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast(device_type=device.type, dtype=torch.float16):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            batch_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # Her 2 epoch'ta bir validation\n",
        "        if epoch % 2 == 0:\n",
        "            val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
        "            scheduler.step(val_loss)\n",
        "        else:\n",
        "            val_loss, val_acc = None, None\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
        "              f\"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f} \"\n",
        "              f\"{f'Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f}' if val_loss is not None else '(Validation skipped)'}\")\n",
        "\n",
        "        if val_loss is not None:\n",
        "            early_stopping(val_loss, model)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping triggered. Stopping training.\")\n",
        "                break\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return train_losses, train_accs, val_losses, val_accs"
      ],
      "id": "BUYXkURTbTXe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d3418fa",
      "metadata": {
        "id": "2d3418fa"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_loader, device, model_path=\"/content/drive/MyDrive/best_model.pth\"):\n",
        "    if os.path.exists(model_path):\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(f\"Best model loaded : {model_path}\")\n",
        "    else:\n",
        "        print(\"Model not found, using current model state.\")\n",
        "\n",
        "    test_loss, test_acc, test_preds, test_labels = evaluate_model(model, test_loader, nn.CrossEntropyLoss(), device)\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    return test_preds, test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9a80dc1",
      "metadata": {
        "id": "b9a80dc1"
      },
      "source": [
        "### 9) Training and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df7e13c3",
      "metadata": {
        "id": "df7e13c3"
      },
      "outputs": [],
      "source": [
        "train_losses, train_accs, val_losses, val_accs = train_model(\n",
        "    model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "    device, early_stopping, epochs=10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds, test_labels = test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "973OsCN595Ga"
      },
      "id": "973OsCN595Ga",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d817ba32",
      "metadata": {
        "id": "d817ba32"
      },
      "source": [
        "### 10) Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b864b0",
      "metadata": {
        "id": "51b864b0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def report_results(preds, labels, class_names=None):\n",
        "    # Tensorları numpy array'e çevir\n",
        "    preds = preds.cpu().numpy() if hasattr(preds, 'cpu') else preds\n",
        "    labels = labels.cpu().numpy() if hasattr(labels, 'cpu') else labels\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    print(classification_report(labels, preds, target_names=class_names))"
      ],
      "metadata": {
        "id": "OEDCQyymOFAH"
      },
      "id": "OEDCQyymOFAH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "\n",
        "report_results(test_preds, test_labels, class_names)\n"
      ],
      "metadata": {
        "id": "W6zaXPXeOHnW"
      },
      "id": "W6zaXPXeOHnW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veri yükleme hızını test et\n",
        "import time\n",
        "start = time.time()\n",
        "for i, (images, labels) in enumerate(train_loader):\n",
        "    if i == 5:  # İlk 5 batch\n",
        "        break\n",
        "    print(f\"Batch {i+1} loaded in {time.time() - start:.2f}s\")\n",
        "    start = time.time()"
      ],
      "metadata": {
        "id": "LUBOAOfkb6nG"
      },
      "id": "LUBOAOfkb6nG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}